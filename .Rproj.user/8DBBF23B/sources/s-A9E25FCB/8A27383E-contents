---
title: "QM 2020 Week 10 -- Maximum Likelihood Estimation"
author: "Marcel Neunhoeffer & Oliver Rittmann"
date: "December 02 & 03, 2020"
output:
  html_document:
    toc: true
    toc_float: true
    css: css/lab.css
  pdf_document:
    toc: yes
  html_notebook:
    toc: true
    toc_float: true
    css: css/lab.css
---

---

  
## Today:

1.  MLE: Binomial Distribution
2.  MLE: LM Without Covariates
3.  MLE: LM With One Covariate

## Goals for Today:

- Understand the likelihood principle.
- Calculate and maximize (log-) likelihood functions in R.

---

```{r setup}
# The first line sets an option for the final document that can be produced from
# the .Rmd file. Don't worry about it.
knitr::opts_chunk$set(echo = TRUE)

# The next bit is quite powerful and useful. 
# First you define which packages you need for your analysis and assign it to 
# the p_needed object. 
p_needed <-
  c("foreign", "viridis", "MASS", "optimx")

# Now you check which packages are already installed on your computer.
# The function installed.packages() returns a vector with all the installed 
# packages.
packages <- rownames(installed.packages())
# Then you check which of the packages you need are not installed on your 
# computer yet. Essentially you compare the vector p_needed with the vector
# packages. The result of this comparison is assigned to p_to_install.
p_to_install <- p_needed[!(p_needed %in% packages)]
# If at least one element is in p_to_install you then install those missing
# packages.
if (length(p_to_install) > 0) {
  install.packages(p_to_install)
}
# Now that all packages are installed on the computer, you can load them for
# this project. Additionally the expression returns whether the packages were
# successfully loaded.
sapply(p_needed, require, character.only = TRUE)

# This is an option for stargazer tables
# It automatically adapts the output to html or latex,
# depending on whether we want a html or pdf file
stargazer_opt <- ifelse(knitr::is_latex_output(), "latex", "html")
```


## 1. MLE: Binomial Distribution


Today we will significantly extend our statistical toolbox. Maximum Likelihood Estimation will be really useful to solve many problems (You still should be able to apply OLS though ;-) ).


### Let's start with a simple example: A biased coin

Let's say we have a biased coin, but we do not know the amount of the bias. The only possibility to learn about the bias is to observe how the coin behaves.

We start by tossing the coin ten times and get **3 tails** and **7 heads**. How do we use this data to learn about the bias of the coin?

We are now in a situation where we have empirical data (3 tails, 7 heads), but the probability P(head) is unknown. This is where the concept of **likelihood** comes in. To learn about p, we ask may start by asking: *How likely is it that we observed 3 tails and 7 heads if p was $p = 0.6$?*

The good news is that we know how to calculate this since week 2. If $p=0.6$, then the PMF of the binomial distribution tells us:

$$
f(k = 7, N = 10, p = 0.6) = \frac{N!}{(N-k)!k!}p^k(1-p)^{N-k}
$$

Plugging in yields:

$$
f(k = 7, N = 10, p = 0.6) = \frac{10!}{(10-7)!7!}0.6^y(1-0.6)^{10-0.6}
$$

Let's do the calculus in R:

```{r}
lik_1 <- factorial(10) / (factorial(10 - 7) * factorial(7)) * 0.6 ^ 7 * (1 - 0.6) ^ (10 - 7)
lik_1
```

If P(head) was 0.6, then the probability of observing 7 heads out of 10 coin tosses is 0.22. 

But remember that p is unknown, so it could also be the case that the true probability of p is 0.55 or 0.65 or any other value between 0 and 1. Let's see what happens if we assume that $p = 0.7$:

```{r}
lik_2 <- factorial(10) / (factorial(10 - 7) * factorial(7)) * 0.65 ^ 7 * (1 - 0.65) ^ (10 - 7)
lik_2
```

The resulting likelihood is higher, so it is more likely that the true value of P(head) is 0.65 rather than 0.6. We can nicely see this in a plot:

```{r}
p <- c(0.6, 0.65)

lik <- c(lik_1, lik_2)

plot(x = p,
     y = lik,
     xlim = c(0,1),
     ylim = c(0, 0.3),
     pch = 19,
     col = viridis(2)[1],
     ylab = "L(p)",
     xlab = "p",
     las = 1,
     xaxt = "n",
    bty = "n",
    type = "n")
axis(1, seq(0, 1, 0.1))
segments(x0 = p,
         x1 = p,
         y0 = 0,
         y1 = lik,
         col = viridis(3)[2])
segments(x0 = 0,
         x1 = p,
         y0 = lik,
         y1 = lik,
         col = viridis(3)[2])
points(x = p,
     y = lik,
     xlim = c(0,1),
     ylim = c(0, 0.3),
     pch = 19,
     col = viridis(2)[1])
text(x = 0,
     y = lik + 0.01,
     labels = c(paste("Probability of observing 7 heads if p was ", p)),
     col = viridis(3)[2],
     cex = 0.7,
     pos = 4)

```

The likelihood is the probability of observing some given data depending on the unknown parameter $p$. By now, we know that $L(p = 0.65) > L(p = 0.6)$, but we want to find the value of $p$ that maximizes the likellihood function $L(p)$.

The likelihood function is just like the PMF of the binomial distribution, only that now p is unkown while N (10 coin tosses) and y (seven heads) are known. You also find this function on slide 12. 

$$
L(p|y,N) = \frac{N!}{(N-y)!y!}p^y(1-p)^{N-y}
$$

N = Number of trials (coin tosses)
y = Number of successes ('heads')
p = probability of success (P(head))

We can implement this function in R:

```{r}
binom_lik <- function(y, n, p) {
  factorial(n) / (factorial(n - y) * factorial(y)) * p ^ y * (1 - p) ^ (n - y)
}
```

and plug in various values. 

```{r}
# Let's start with a sequence
p <- seq(0, 1, 0.05)

lik <- binom_lik(y = 7, n = 10, p = p)

plot(
  x = p,
  y = lik,
  xlim = c(0, 1),
  ylim = c(0, 0.3),
  pch = 19,
  col = viridis(2)[1],
  ylab = "L(p)",
  xlab = "p",
  las = 1,
  xaxt = "n",
  bty = "n",
  type = "n"
)
axis(1, seq(0, 1, 0.1))
segments(
  x0 = p,
  x1 = p,
  y0 = 0,
  y1 = lik,
  col = viridis(3)[2]
)
segments(
  x0 = 0,
  x1 = p[9:15],
  y0 = lik[9:15],
  y1 = lik[9:15],
  col = viridis(3)[2]
)
points(
  x = p,
  y = lik,
  xlim = c(0, 1),
  ylim = c(0, 0.3),
  pch = 19,
  col = viridis(2)[1]
)
text(
  x = 0,
  y = lik[9:15] + 0.005,
  labels = c(paste(
    "Probability of observing 7 heads if p was ", p[9:15]
  )),
  col = viridis(3)[2],
  cex = 0.7,
  pos = 4
)
```

Because p can take any value between 0 and 1, we are dealing with a continuous distribution. Let's plot this and find the value of $p$ that maximizes the likelihood function (i.e. the value of $p$ that is *most likely given the observed data*)

```{r}
# Let's start with a sequence
p <- seq(0, 1, 0.01)

lik <- binom_lik(y = 7, n = 10, p = p)

# which value of p maximizes the likelihood fanction?
p_max_lik <- p[which.max(lik)]
max_lik <- lik[which.max(lik)]

plot(x = p,
     y = lik,
     xlim = c(0, 1),
     ylim = c(0, 0.3),
     pch = 19,
     col = viridis(2)[1],
     ylab = "L(p)",
     xlab = "p",
     type = "n",
     las = 1,
     xaxt = "n",
     bty = "n")
axis(1, seq(0, 1, 0.1))
segments(x0 = p_max_lik,
         x1 = p_max_lik,
         y0 = 0,
         y1 = max_lik,
         col = viridis(3)[2],
         lwd = 2)
segments(x0 = 0,
         x1 = p_max_lik,
         y0 = max_lik,
         y1 = max_lik,
         col = viridis(3)[2],
         lwd = 2)
lines(x = p,
     y = lik,
     xlim = c(0,1),
     ylim = c(0, 0.3),
     pch = 19,
     col = viridis(2)[1])
text(x = 0,
     y = max_lik + 0.01,
     labels = "Maximum likelihood estimate: p = 0.7",
     col = viridis(3)[2],
     cex = 0.7,
     pos = 4)
```

---

## Exercise section: Example from the lecture

Let's do this again for the example from the lecture: out of 10 students, 6 passed.

We are again dealing with a binomial process, so the likelihood function is the same:. 

$$
L(p|y,N) = \frac{N!}{(N-y)!y!}p^y(1-p)^{N-y}
$$

```{r}
binom_lik <- function(y, n, p) {
  factorial(n) / (factorial(n - y) * factorial(y)) * p ^ y * (1 - p) ^ (n - y)
}
```

Your task: find the value of $p$ that maximizes the likelihood function. To achieve this, you need to do the following steps:

  - Create a vector with possible values for $p$.
  - Use the likelihood function to calculate the likelihood of $p$ given our data (y = 6, n = 10).
  - Look for the p-value that maximizes the Likelihood function.
  

```{r}

# 1) Create a vector with possible values for p. Name the vector p.


# 2) Calculate likelihood of p given our data (y = 6, n = 10).
# The object holding the results should be named res.


# 3) Look for the p-value that maximizes the Likelihood function.
# Name the object max_lik_p

```

  - What do you think is the probability of passing the exam?
  - Extra question: What would you have to do to solve this exercise analytically?


If you did everything correctly (and named your objects accordingly), you can plot your results

```{r, eval = F}
plot(
  p,
  res,
  type = "l",
  las = 1,
  bty = "n",
  ylab = "L(p)",
  col = viridis(2)[1]
)
abline(v = max_lik_p, 
       col = viridis(2)[2],
       lwd = 2)
```
**End of exercise.**

---

Of course there are also function libraries to search for the Maximum value of the Likelihood function.

For one-dimensional problems, we can Maximize the Likelihood function using optimize(). Disclaimer: We will usually not maximize one-dimensional likelihood functions from now on.

```{r}
res_opt <-
  optimize(
    f = binom_lik,
    # f is the function we want to optimize (here: maximize).
    interval = c(0, 1),
    # we need to specify an interval of values where we want to look for the maximum
    y = 6,
    n = 10,
    # y and n is the data we actually observed
    maximum = T
    # finally, we need to tell the function that we are looking for a maximum.
  )

res_opt
```


```{r, eval = F}
plot(
  p,
  res,
  type = "l",
  las = 1,
  bty = "n",
  ylab = "L(p)",
  col = viridis(3)[1]
)
abline(v = max_lik_p, 
       col = viridis(3)[2],
       lwd = 2)
abline(v = res_opt$maximum,
       col = viridis(3)[3],
       lwd = 2,
       lty = "dashed")
legend("topleft",
       col = c(viridis(3)[1],
               viridis(3)[2],
               viridis(3)[3]),
       lty = c("solid", 
               "solid", 
               "dashed"),
       legend = c("Likelihood function",
                  "max(Likelihood), by hand",
                  "max(Likelihood), optimize()"),
       bty = "n",
       cex = 0.85)
```


### Log-Likelihood

In practice, the log of the likelihood is usually used. 
Why? Taking the logarithm does not change the maximum of the function but changes products to sums and thus makes calculations easier.


Step 1: Write down (a.k.a. translate from the slides) the Log-Likelihood function from slide 10:

$$
\log L(p| y, N) = \log\left[\frac{N!}{(N-y)! y!}\right] + y \log p + (N-y)\: \log (1-p)
$$

```{r}
binom_loglik <- function(y, n, p) {
  log(factorial(n) / (factorial(n - y) * factorial(y))) + 
    y * log(p) + (n - y) * log(1 - p)
}
```

Step 2: Maximize the function using optimize.

```{r}
res <- optimize(
  f = binom_loglik,
  interval = c(0, 1),
  y = 6,
  n = 10,
  maximum = T
)
```

Step 3: Also make a plot of the Log-Likelihood function. And include a line of the maximum likelihood estimate.

```{r}
p <- seq(0, 1, length.out = 1000)

res_loglik <- binom_loglik(6, 10, p)

#This gives the graph from Slide 14 in the lecture:

plot(
  p,
  res_loglik,
  type = "l",
  ylim = c(-10, 0),
  las = 1,
  bty = "n",
  ylab = "logL(p)",
  col = viridis(2)[1]
)
abline(v = res$maximum,
       col = viridis(2)[2],
       lwd = 2,
       lty = "dashed")
```

We could also compare the two log-likelihood functions from Slide 10, to show that they lead to the same result:

$$
\log L(p| y, N) = y\, \log p + (N-y)\: \log (1-p)\\ 
$$

```{r}
#From Slide 11:

binom_loglik2 <- function(y, n, p) {
  y * log(p) + (n - y) * log(1 - p)
  
}

p <- seq(0, 1, length.out = 1000)

res2 <- binom_loglik2(6, 10, p)
max_loglik_p2 <- p[which.max(res2)]

plot(
  p,
  res_loglik,
  type = "l",
  ylim = c(-20, 0),
  las = 1,
  bty = "n",
  ylab = "logL(p)",
  col = viridis(3)[1]
)
abline(v = res$maximum, 
       lwd = 2,
       col = viridis(3)[2])
lines(x = p, 
      y = res2, 
      lty = "dashed",
      col = viridis(3)[1])
abline(v = max_loglik_p2, 
       lwd = 2,
       lty = "dashed",
       col = viridis(3)[3])

legend("topleft",
       col = c(viridis(3)[1],
               viridis(3)[1],
               viridis(3)[2],
               viridis(3)[3]),
       lty = c("solid", 
               "dashed", 
               "solid", 
               "dashed"),
       legend = c("LogLik 1",
                  "LogLik 2 (simplified)",
                  "max(LogLik 1)",
                  "max(LogLik 2)"),
       bty = "n",
       cex = 0.75)
```

Both likelihood function obviously lead to the same p, but are shifted.

---

## 2. MLE: Linear Model Without Covariates

As we did many times before, we start with some fake data:

We set the true parameter values.

```{r}
b0 <- 5
sigma2 <- 4
```

Then we generate a dependent variable. We only have an intercept and random noise.

```{r}
Y <- b0 + rnorm(100000, 0, sqrt(sigma2))
```

Now we want to write down the Log-Likelihood function for a linear model without any covariates. (Have a look at slide 18.). 

$$
log L(\beta_0, \sigma^2) = -\frac{N}{2} log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i - \beta_0)^2
$$

Optimizing routines need a vector with all the parameters. This is theta.

```{r}
lmsimple_loglik <- function(y, theta) {
  N <- length(y)
  
  b0 <- theta[1]
  sigma2 <-
    exp(theta[2]) #constrain variance so that it is always positive
  
  logl <-
    -(N / 2) * log(sigma2) - 1 / (2 * sigma2) * sum((y - b0) ^ 2)
  
  return(logl)
}
```

Since we are leaving the one-dimensional world now, we start to use some new optimizing routine. We use the `optimx` package.

In order for `optimx` to find the peak of mount likelihood we need to tell it were to start.

```{r}
stval <- c(0, 0)
```

Now we can optimize the Log-Likelihood function.

```{r}
res <-
  optimx(
    par = stval,
    # we need to input our start values,
    f = lmsimple_loglik,
    # the Log-Likelihood function we optimize,
    y = Y,
    # our data,
    control = list(maximize = T)
    # and tell optimx to maximize rather than to minimize.
  )  
```

There are other packages for optimization out there. If you want you can experiment with them: e.g. maxLik, optim

```{r}
res
mean(Y)
```

Why is $\sigma^2 \approx `r round(res$p2[1],2)`$?

```{r}
log(4)

exp(res$p2[1])

```

If we hold $\sigma^2$ fixed, we can make a (2D) plot of the log likelihood function.

```{r}
b <- seq(-10, 10, length.out = 1000)

res <-
  sapply(b, function(x)
    lmsimple_loglik(y = Y, theta = cbind(x, sigma2)))

min_loglik_b <- b[which.max(res)]

plot(
  b,
  res,
  type = "l",
  bty = "n",
  ylab = "logL",
  xlab = "beta",
  yaxt = "n",
  col = viridis(2)[1],
)
axis_ticks <- pretty(min(res):max(res))
axis(2,
     at = axis_ticks,
     labels = paste0(axis_ticks/1000, "k"),
     las = 1)
abline(v = min_loglik_b,
       lwd = 2,
       col = viridis(2)[2])
```

## 3. MLE: Linear Model With One Covariate

Let's load some very familiar data.

```{r}
dat <- read.dta("raw-data/uspresidentialelections.dta")
```

To make it easier to write our log-likelihood function we specify our variables as Y and X.

```{r}
Y <- dat$vote
X <- dat$growth
```

Let's estimate our simple bivariate model using Maximum Likelihood Estimation.

### Example II: Implement a maximum likelihood estimation with one covariate.

First, we modify our log-likelihood function from above to include one covariate.
Any ideas how we can modify it?


$$
log L(\beta_0, \beta_1, \sigma^2) = -\frac{N}{2} log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^{N} (y_i - \beta_0 - \beta_1 x_i)^2
$$

```{r}
lm_loglik <- function(y, x, theta) {
  N <- length(y)
  
  # theta contains our parameters to be estimated
  
  beta0 <- theta[1]
  beta1 <- theta[2]
  sigma2 <- exp(theta[3])
  
  logl <-
    -N / 2 * log(sigma2) - 1 / (2 * sigma2) * sum((y - beta0 - beta1 * x) ^ 2)
  return(logl)
}
```

---

Now we can optimize!

We now need 3 starting values. 

**What are the 3 values?**

```{r}
stval <- c(0, 0, 0)
```

Now we have everything (a log-likelihood function we want to maximize and starting values) that we need to optimize.

```{r}
res <-
  optimx(
    stval,
    lm_loglik,
    y = Y,
    x = X,
    control = list(maximize = T)
  )

res
```


Finally, we want to know whether MLE gives us similar results to what OLS would have told us.

```{r}
ols <- lm(Y ~ X)

ols
```

It's looking good.


We can easily extend the model to 2 or more covariates.
However, it is convenient to introduce some matrix notation for that.
We will do that in Advanced Quantitative Methods next spring!


## Concluding Remarks


In your Homework for this week you will:

*  Estimate the likelihood to pass an exam using maximum likelihood.
*  Re-visit a data set you already know and compare the results of a OLS regression with the results of a ML regression.
